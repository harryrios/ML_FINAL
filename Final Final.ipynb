{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries to be used:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import svm\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n",
       "       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('train (1).csv')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting mean age\n",
    "mean_age = df['Age'].mean() # 29.699...\n",
    "df['Age'] = df['Age'].fillna(mean_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Survived']\n",
    "X = df[['Age', 'SibSp','Parch','Fare']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = df['Name']\n",
    "titles = ['Mr.', 'Mrs.', 'Miss.', 'Master.', 'Don.', 'Rev.', 'Dr.', 'Mme.', 'Ms.',\n",
    "       'Major.', 'Lady.', 'Sir.', 'Mlle.', 'Col.', 'Capt.', 'Countess.',\n",
    "       'Jonkheer.']\n",
    "\n",
    "is_Mr = []\n",
    "is_Mrs = []\n",
    "is_Miss = []\n",
    "is_Master = []\n",
    "is_Other = []\n",
    "\n",
    "for name in names:\n",
    "    for title in titles:\n",
    "        if title in name:\n",
    "            if title == 'Mr.':\n",
    "                is_Other.append(0)\n",
    "                is_Mr.append(1)\n",
    "                is_Mrs.append(0)\n",
    "                is_Miss.append(0)\n",
    "                is_Master.append(0)\n",
    "                \n",
    "            elif title == 'Mrs.':\n",
    "                is_Other.append(0)\n",
    "                is_Mr.append(0)\n",
    "                is_Mrs.append(1)\n",
    "                is_Miss.append(0)\n",
    "                is_Master.append(0)\n",
    "                \n",
    "            elif title == 'Miss.':\n",
    "                is_Other.append(0)\n",
    "                is_Mr.append(0)\n",
    "                is_Mrs.append(0)\n",
    "                is_Miss.append(1)\n",
    "                is_Master.append(0)\n",
    "                \n",
    "            elif title == 'Master.':\n",
    "                is_Other.append(0)\n",
    "                is_Mr.append(0)\n",
    "                is_Mrs.append(0)\n",
    "                is_Miss.append(0)\n",
    "                is_Master.append(1)\n",
    "                \n",
    "            else:\n",
    "                is_Other.append(1)\n",
    "                is_Mr.append(0)\n",
    "                is_Mrs.append(0)\n",
    "                is_Miss.append(0)\n",
    "                is_Master.append(0)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n",
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n",
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "is_Mr = pd.Series(is_Mr)\n",
    "is_Mrs = pd.Series(is_Mrs)\n",
    "is_Miss = pd.Series(is_Miss)\n",
    "is_Master = pd.Series(is_Master)\n",
    "is_Other = pd.Series(is_Other)\n",
    "\n",
    "X['isMr'] = is_Mr\n",
    "X['isMrs'] = is_Mrs\n",
    "X['isMiss'] = is_Miss\n",
    "X['isMaster'] = is_Master\n",
    "X['isOther'] = is_Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>isMr</th>\n",
       "      <th>isMrs</th>\n",
       "      <th>isMiss</th>\n",
       "      <th>isMaster</th>\n",
       "      <th>isOther</th>\n",
       "      <th>isMale</th>\n",
       "      <th>isFemale</th>\n",
       "      <th>Pclass_1</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "      <th>Embarked_C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Age  SibSp  Parch     Fare  isMr  isMrs  isMiss  isMaster  isOther  \\\n",
       "0  22.0      1      0   7.2500     1      0       0         0        0   \n",
       "1  38.0      1      0  71.2833     0      1       0         0        0   \n",
       "2  26.0      0      0   7.9250     0      0       1         0        0   \n",
       "3  35.0      1      0  53.1000     0      1       0         0        0   \n",
       "4  35.0      0      0   8.0500     1      0       0         0        0   \n",
       "\n",
       "   isMale  isFemale  Pclass_1  Pclass_2  Pclass_3  Embarked_Q  Embarked_S  \\\n",
       "0       1         0         0         0         1           0           1   \n",
       "1       0         1         1         0         0           0           0   \n",
       "2       0         1         0         0         1           0           1   \n",
       "3       0         1         1         0         0           0           1   \n",
       "4       1         0         0         0         1           0           1   \n",
       "\n",
       "   Embarked_C  \n",
       "0           0  \n",
       "1           1  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OHE = One Hot Encoding\n",
    "OHE_SEX = pd.get_dummies(df.Sex, prefix='Sex')\n",
    "OHE_PCLASS = pd.get_dummies(df.Pclass, prefix='Pclass')\n",
    "OHE_EMB = pd.get_dummies(df.Embarked, prefix='Embarked')\n",
    "\n",
    "X['isMale'] = OHE_SEX['Sex_male']\n",
    "X['isFemale'] = OHE_SEX['Sex_female']\n",
    "\n",
    "X['Pclass_1'] = OHE_PCLASS['Pclass_1']\n",
    "X['Pclass_2'] = OHE_PCLASS['Pclass_2']\n",
    "X['Pclass_3'] = OHE_PCLASS['Pclass_3']\n",
    "\n",
    "X['Embarked_Q'] = OHE_EMB['Embarked_Q']\n",
    "X['Embarked_S'] = OHE_EMB['Embarked_S']\n",
    "X['Embarked_C'] = OHE_EMB['Embarked_C']\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we can do Z standarization\n",
    "\n",
    "y = df['Survived']\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X)\n",
    "\n",
    "X = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's split our data into train and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Now that our data is preprocessed, we can begin to run our models\n",
    "#\n",
    "#  The first will be Logistical Regression, L1 and L2 with and without polynomial data\n",
    "#  The second will be SVMs with linear, rbf, and polynomial kernels\n",
    "#  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistical Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 1 : L1 logistical regression\n",
    "\n",
    "def logreg_model(c , X_train, Y_train, X_test, Y_test):\n",
    "    \n",
    "    logreg = linear_model.LogisticRegression(penalty = 'l1', C=c, solver = 'saga', max_iter=10000)\n",
    "    logreg.fit(X_train, Y_train)\n",
    "\n",
    "    Yhat_train = logreg.predict(X_train) \n",
    "    acc_train = logreg.score(X_train, Y_train)\n",
    "    \n",
    "    Yhat_test = logreg.predict(X_test)\n",
    "    acc_test = logreg.score(X_test, Y_test)\n",
    "    \n",
    "    print('C = ', c)\n",
    "    print(\"Accuracy on training data = %f\" % acc_train)\n",
    "    print(\"Accuracy on test data = %f\" % acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C =  0.0001\n",
      "Accuracy on training data = 0.627517\n",
      "Accuracy on test data = 0.593220\n",
      "C =  0.001\n",
      "Accuracy on training data = 0.627517\n",
      "Accuracy on test data = 0.593220\n",
      "C =  0.01\n",
      "Accuracy on training data = 0.627517\n",
      "Accuracy on test data = 0.593220\n",
      "C =  0.1\n",
      "Accuracy on training data = 0.822148\n",
      "Accuracy on test data = 0.823729\n",
      "C =  1\n",
      "Accuracy on training data = 0.832215\n",
      "Accuracy on test data = 0.823729\n",
      "C =  10\n",
      "Accuracy on training data = 0.835570\n",
      "Accuracy on test data = 0.823729\n",
      "C =  100\n",
      "Accuracy on training data = 0.832215\n",
      "Accuracy on test data = 0.823729\n"
     ]
    }
   ],
   "source": [
    "cVals = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]\n",
    "for c in cVals:\n",
    "    logreg_model(c, X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 2 : L2 logistical regression\n",
    "\n",
    "def logreg2_model(c , X_train, Y_train, X_test, Y_test):\n",
    "\n",
    "    logreg = linear_model.LogisticRegression( C=c, max_iter=10000)\n",
    "    logreg.fit(X_train, Y_train)\n",
    "\n",
    "    Yhat_train = logreg.predict(X_train) \n",
    "    acc_train = logreg.score(X_train, Y_train)\n",
    "    \n",
    "    Yhat_test = logreg.predict(X_test)\n",
    "    acc_test = logreg.score(X_test, Y_test)\n",
    "    \n",
    "    print('C = ', c)\n",
    "    print(\"Accuracy on training data = %f\" % acc_train)\n",
    "    print(\"Accuracy on test data = %f\" % acc_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C =  0.0001\n",
      "Accuracy on training data = 0.627517\n",
      "Accuracy on test data = 0.593220\n",
      "C =  0.001\n",
      "Accuracy on training data = 0.770134\n",
      "Accuracy on test data = 0.762712\n",
      "C =  0.01\n",
      "Accuracy on training data = 0.825503\n",
      "Accuracy on test data = 0.823729\n",
      "C =  0.1\n",
      "Accuracy on training data = 0.832215\n",
      "Accuracy on test data = 0.820339\n",
      "C =  1\n",
      "Accuracy on training data = 0.835570\n",
      "Accuracy on test data = 0.823729\n",
      "C =  10\n",
      "Accuracy on training data = 0.832215\n",
      "Accuracy on test data = 0.823729\n",
      "C =  100\n",
      "Accuracy on training data = 0.832215\n",
      "Accuracy on test data = 0.823729\n"
     ]
    }
   ],
   "source": [
    "cVals = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]\n",
    "for c in cVals:\n",
    "    logreg2_model(c, X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try again with polynomial transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(2)\n",
    "X_transformed_train = poly.fit_transform(X_train)\n",
    "X_transformed_test = poly.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C =  0.0001\n",
      "Accuracy on training data = 0.627517\n",
      "Accuracy on test data = 0.593220\n",
      "C =  0.001\n",
      "Accuracy on training data = 0.627517\n",
      "Accuracy on test data = 0.593220\n",
      "C =  0.01\n",
      "Accuracy on training data = 0.719799\n",
      "Accuracy on test data = 0.691525\n",
      "C =  0.1\n",
      "Accuracy on training data = 0.852349\n",
      "Accuracy on test data = 0.827119\n",
      "C =  1\n",
      "Accuracy on training data = 0.854027\n",
      "Accuracy on test data = 0.810169\n",
      "C =  10\n",
      "Accuracy on training data = 0.855705\n",
      "Accuracy on test data = 0.816949\n",
      "C =  100\n",
      "Accuracy on training data = 0.855705\n",
      "Accuracy on test data = 0.813559\n"
     ]
    }
   ],
   "source": [
    "cVals = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]\n",
    "for c in cVals:\n",
    "    logreg_model(c, X_transformed_train, Y_train, X_transformed_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C =  0.0001\n",
      "Accuracy on training data = 0.635906\n",
      "Accuracy on test data = 0.613559\n",
      "C =  0.001\n",
      "Accuracy on training data = 0.835570\n",
      "Accuracy on test data = 0.827119\n",
      "C =  0.01\n",
      "Accuracy on training data = 0.850671\n",
      "Accuracy on test data = 0.830508\n",
      "C =  0.1\n",
      "Accuracy on training data = 0.850671\n",
      "Accuracy on test data = 0.816949\n",
      "C =  1\n",
      "Accuracy on training data = 0.859060\n",
      "Accuracy on test data = 0.813559\n",
      "C =  10\n",
      "Accuracy on training data = 0.860738\n",
      "Accuracy on test data = 0.806780\n",
      "C =  100\n",
      "Accuracy on training data = 0.860738\n",
      "Accuracy on test data = 0.806780\n"
     ]
    }
   ],
   "source": [
    "cVals = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]\n",
    "for c in cVals:\n",
    "    logreg2_model(c, X_transformed_train, Y_train, X_transformed_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_linear(c, X_train, Y_train, X_test, Y_test):\n",
    "    \n",
    "    svc_linear = svm.SVC(probability = False, kernel = 'linear', C = c)\n",
    "    svc_linear.fit(X_train, Y_train)\n",
    "    \n",
    "    Yhat_svc_linear_train = svc_linear.predict(X_train)\n",
    "    \n",
    "    acc_train = svc_linear.score(X_train, Y_train)\n",
    "    acc_test = svc_linear.score(X_test, Y_test)\n",
    "    \n",
    "    print('C = ', c)\n",
    "    print('Train Accuracy = {0:f}'.format(acc_train))\n",
    "    print('Test Accuracy = {0:f}'.format(acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C =  0.0001\n",
      "Train Accuracy = 0.627517\n",
      "Test Accuracy = 0.593220\n",
      "C =  0.001\n",
      "Train Accuracy = 0.786913\n",
      "Test Accuracy = 0.796610\n",
      "C =  0.01\n",
      "Train Accuracy = 0.805369\n",
      "Test Accuracy = 0.833898\n",
      "C =  0.1\n",
      "Train Accuracy = 0.825503\n",
      "Test Accuracy = 0.840678\n",
      "C =  1\n",
      "Train Accuracy = 0.827181\n",
      "Test Accuracy = 0.833898\n",
      "C =  10\n",
      "Train Accuracy = 0.827181\n",
      "Test Accuracy = 0.833898\n",
      "C =  100\n",
      "Train Accuracy = 0.827181\n",
      "Test Accuracy = 0.833898\n"
     ]
    }
   ],
   "source": [
    "cVals = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]\n",
    "for c in cVals:\n",
    "    svm_linear(c, X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_rbf(c, X_train, Y_train, X_test, Y_test):\n",
    "    print('C = ', c)\n",
    "\n",
    "    svc_rbf = svm.SVC(probability = False, kernel = 'rbf', C = c)\n",
    "    \n",
    "    svc_rbf.fit(X_train, Y_train)\n",
    "    \n",
    "    acc_train = svc_rbf.score(X_train,Y_train)\n",
    "    print('Train Accuracy = {0:f}'.format(acc_train))\n",
    "\n",
    "    acc_test = svc_rbf.score(X_test,Y_test)\n",
    "    print('Test Accuracy = {0:f}'.format(acc_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C =  0.0001\n",
      "Train Accuracy = 0.627517\n",
      "Test Accuracy = 0.593220\n",
      "C =  0.001\n",
      "Train Accuracy = 0.627517\n",
      "Test Accuracy = 0.593220\n",
      "C =  0.01\n",
      "Train Accuracy = 0.627517\n",
      "Test Accuracy = 0.593220\n",
      "C =  0.1\n",
      "Train Accuracy = 0.833893\n",
      "Test Accuracy = 0.844068\n",
      "C =  1\n",
      "Train Accuracy = 0.850671\n",
      "Test Accuracy = 0.830508\n",
      "C =  10\n",
      "Train Accuracy = 0.864094\n",
      "Test Accuracy = 0.810169\n",
      "C =  100\n",
      "Train Accuracy = 0.890940\n",
      "Test Accuracy = 0.803390\n"
     ]
    }
   ],
   "source": [
    "cVals = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]\n",
    "for c in cVals:\n",
    "    svm_rbf(c, X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_polynomial(c, X_train, Y_train, X_test, Y_test):\n",
    "    print('C = ', c)\n",
    "    \n",
    "    svc_polynomial = svm.SVC(probability = False, kernel = 'poly', C = c) \n",
    "    svc_polynomial.fit(X_train, Y_train)\n",
    "\n",
    "    Yhat_svc_poly_train = svc_polynomial.predict(X_train)\n",
    "    acc_train = svc_polynomial.score(X_train, Y_train)\n",
    "    \n",
    "    print('Train Accuracy = {0:f}'.format(acc_train))\n",
    "    \n",
    "    Yhat_svc_poly_test = svc_polynomial.predict(X_test)\n",
    "    acc_test = svc_polynomial.score(X_test, Y_test)\n",
    "\n",
    "    print('Test Accuracy = {0:f}'.format(acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C =  0.0001\n",
      "Train Accuracy = 0.627517\n",
      "Test Accuracy = 0.593220\n",
      "C =  0.001\n",
      "Train Accuracy = 0.629195\n",
      "Test Accuracy = 0.593220\n",
      "C =  0.01\n",
      "Train Accuracy = 0.640940\n",
      "Test Accuracy = 0.610169\n",
      "C =  0.1\n",
      "Train Accuracy = 0.843960\n",
      "Test Accuracy = 0.833898\n",
      "C =  1\n",
      "Train Accuracy = 0.854027\n",
      "Test Accuracy = 0.830508\n",
      "C =  10\n",
      "Train Accuracy = 0.870805\n",
      "Test Accuracy = 0.800000\n",
      "C =  100\n",
      "Train Accuracy = 0.879195\n",
      "Test Accuracy = 0.793220\n"
     ]
    }
   ],
   "source": [
    "cVals = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]\n",
    "for c in cVals:\n",
    "    svm_polynomial(c, X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.924496644295302\n",
      "0.752542372881356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    }
   ],
   "source": [
    "clf = MLPClassifier(solver = 'lbfgs', max_iter = 500, hidden_layer_sizes = (25))\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "train_score = clf.score(X_train, Y_train)\n",
    "test_score = clf.score(X_test, Y_test)\n",
    "\n",
    "print(train_score)\n",
    "print(test_score)\n",
    "\n",
    "# with clf = MLPClassifier(solver = 'lbfgs')\n",
    "# train = .93\n",
    "# test = .77\n",
    "\n",
    "\n",
    "# 1. How about larger alpha?\n",
    "#---------------------------------\n",
    "# with clf = MLPClassifier(solver = 'lbfgs', max_iter = 500, alpha = .001)\n",
    "# train = .95\n",
    "# test = .77\n",
    "\n",
    "# with clf = clf = MLPClassifier(solver = 'lbfgs', max_iter = 500, alpha = .01)\n",
    "# train = .95\n",
    "# test = .77\n",
    "\n",
    "# 2. How about less hidden neurons?\n",
    "#-------------------------------------\n",
    "# with clf = MLPClassifier(solver = 'lbfgs', hidden_layer_sizes = (50))\n",
    "# train = .92\n",
    "# test = .78\n",
    "\n",
    "# with clf = MLPClassifier(solver = 'lbfgs', hidden_layer_sizes = (25))\n",
    "# train = .91\n",
    "# test = .79\n",
    "\n",
    "# with clf = MLPClassifier(solver = 'lbfgs', hidden_layer_sizes = (10))\n",
    "# train = .89\n",
    "# test = .78\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8187919463087249\n",
      "0.8067796610169492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "clf = MLPClassifier(solver = 'sgd', hidden_layer_sizes = (10))\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "train_score = clf.score(X_train, Y_train)\n",
    "test_score = clf.score(X_test, Y_test)\n",
    "\n",
    "print(train_score)\n",
    "print(test_score)\n",
    "\n",
    "\n",
    "# with clf = MLPClassifier(solver = 'sgd')\n",
    "# train = .86\n",
    "# test  = .82\n",
    "\n",
    "\n",
    "# 1. How about larger alpha?\n",
    "#---------------------------------\n",
    "# with clf = MLPClassifier(solver = 'sgd', alpha = .001)\n",
    "# train = .84\n",
    "# test  = .82\n",
    "\n",
    "# with clf = MLPClassifier(solver = 'sgd', alpha = .01)\n",
    "# train = .84\n",
    "# test  = .81\n",
    "\n",
    "\n",
    "# 2. How about less hidden neurons?\n",
    "#-------------------------------------\n",
    "# with clf = MLPClassifier(solver = 'sgd', hidden_layer_sizes = (50))\n",
    "# train = .83\n",
    "# test  = .82\n",
    "\n",
    "# with clf = MLPClassifier(solver = 'sgd', hidden_layer_sizes = (25))\n",
    "# train = .85\n",
    "# test  = .82\n",
    "\n",
    "# with clf = MLPClassifier(solver = 'sgd', hidden_layer_sizes = (10))\n",
    "# train = .82\n",
    "# test  = .81"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8506711409395973\n",
      "0.8338983050847457\n"
     ]
    }
   ],
   "source": [
    "clf = MLPClassifier(solver = 'adam', hidden_layer_sizes = (10), max_iter = 500)\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "train_score = clf.score(X_train, Y_train)\n",
    "test_score = clf.score(X_test, Y_test)\n",
    "\n",
    "print(train_score)\n",
    "print(test_score)\n",
    "\n",
    "# with clf = MLPClassifier(solver = 'adam')\n",
    "# train = .86\n",
    "# test  = .82\n",
    "\n",
    "\n",
    "# 1. How about larger alpha?\n",
    "#---------------------------------\n",
    "# with clf = MLPClassifier(solver = 'adam', alpha = .001)\n",
    "# train = .86\n",
    "# test  = .82\n",
    "\n",
    "# with clf = MLPClassifier(solver = 'adam', alpha = .01)\n",
    "# train = .86\n",
    "# test  = .82\n",
    "\n",
    "\n",
    "# 2. How about less hidden neurons?\n",
    "#-------------------------------------\n",
    "# with clf = MLPClassifier(solver = 'adam', hidden_layer_sizes = (50))\n",
    "# train = .85\n",
    "# test  = .82\n",
    "\n",
    "# with clf = MLPClassifier(solver = 'adam', hidden_layer_sizes = (25))\n",
    "# train = .85\n",
    "# test  = .82\n",
    "\n",
    "# with clf = MLPClassifier(solver = 'adam', hidden_layer_sizes = (10))\n",
    "# train = .83\n",
    "# test  = .85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From these permutations, it seems like 'adam' solver is the best\n",
    "# According to the scikit Learn website, it is good for large data sets (1k or more examples)\n",
    "\n",
    "# Additionally, changing alpha seems to be pretty useless, but changing the hiddel layers might\n",
    "# be helpful.\n",
    "\n",
    "# In the next round of testing, let's try using 'adam' with different hidden layers and different\n",
    "# acitvation functions.\n",
    "\n",
    "# The default solver is ReLu, so going forward let's try identity, logistic, and tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8322147651006712\n",
      "0.8203389830508474\n"
     ]
    }
   ],
   "source": [
    "clf = MLPClassifier(solver = 'adam', activation = 'identity', hidden_layer_sizes = (10))\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "train_score = clf.score(X_train, Y_train)\n",
    "test_score = clf.score(X_test, Y_test)\n",
    "\n",
    "print(train_score)\n",
    "print(test_score)\n",
    "\n",
    "# with clf = MLPClassifier(solver = 'adam', activation = 'identity')\n",
    "# train = .83\n",
    "# test  = .82\n",
    "\n",
    "\n",
    "# 1. How about less hidden neurons?\n",
    "#-------------------------------------\n",
    "# with clf = MLPClassifier(solver = 'adam', activation = 'identity', hidden_layer_sizes = (50))\n",
    "# train = .83\n",
    "# test  = .82\n",
    "\n",
    "# with clf = MLPClassifier(solver = 'adam', activation = 'identity', hidden_layer_sizes = (25))\n",
    "# train = .84\n",
    "# test  = .82\n",
    "\n",
    "# with clf = MLPClassifier(solver = 'adam', activation = 'identity', hidden_layer_sizes = (10))\n",
    "# train = .83\n",
    "# test  = .82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8305369127516778\n",
      "0.8271186440677966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "clf = MLPClassifier(solver = 'adam', activation = 'logistic', hidden_layer_sizes = (10))\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "train_score = clf.score(X_train, Y_train)\n",
    "test_score = clf.score(X_test, Y_test)\n",
    "\n",
    "print(train_score)\n",
    "print(test_score)\n",
    "\n",
    "# with clf = MLPClassifier(solver = 'adam', activation = 'logistic')\n",
    "# train = .83\n",
    "# test  = .82\n",
    "\n",
    "\n",
    "# 1. How about less hidden neurons?\n",
    "#-------------------------------------\n",
    "# with clf = MLPClassifier(solver = 'adam', activation = 'logistic', hidden_layer_sizes = (50))\n",
    "# train = .83\n",
    "# test  = .83\n",
    "\n",
    "# with clf = MLPClassifier(solver = 'adam', activation = 'logistic', hidden_layer_sizes = (25))\n",
    "# train = .83\n",
    "# test  = .83\n",
    "\n",
    "# with clf = MLPClassifier(solver = 'adam', activation = 'logistic', hidden_layer_sizes = (10))\n",
    "# train = .83\n",
    "# test  = .83"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8406040268456376\n",
      "0.8338983050847457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "clf = MLPClassifier(solver = 'adam', activation = 'tanh', hidden_layer_sizes = (10))\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "train_score = clf.score(X_train, Y_train)\n",
    "test_score = clf.score(X_test, Y_test)\n",
    "\n",
    "print(train_score)\n",
    "print(test_score)\n",
    "\n",
    "# with clf = MLPClassifier(solver = 'adam', activation = 'tanh')\n",
    "# train = .85\n",
    "# test  = .82\n",
    "\n",
    "\n",
    "# 1. How about less hidden neurons?\n",
    "#-------------------------------------\n",
    "# with clf = MLPClassifier(solver = 'adam', activation = 'tanh', hidden_layer_sizes = (50))\n",
    "# train = .85\n",
    "# test  = .82\n",
    "\n",
    "# with clf = MLPClassifier(solver = 'adam', activation = 'tanh', hidden_layer_sizes = (25))\n",
    "# train = .85\n",
    "# test  = .82\n",
    "\n",
    "# with clf = MLPClassifier(solver = 'adam', activation = 'tanh', hidden_layer_sizes = (10))\n",
    "# train = .84\n",
    "# test  = .83"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After looking at these, the best is still:\n",
    "#   clf = MLPClassifier(solver = 'adam', hidden_layer_sizes = (10))\n",
    "\n",
    "# then using these parameters, what if we changed the number of hidden layers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8439597315436241\n",
      "0.8101694915254237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "clf = MLPClassifier(solver = 'adam', hidden_layer_sizes = (10))\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "train_score = clf.score(X_train, Y_train)\n",
    "test_score = clf.score(X_test, Y_test)\n",
    "\n",
    "print(train_score)\n",
    "print(test_score)\n",
    "\n",
    "# with clf = MLPClassifier(solver = 'adam', hidden_layer_sizes = (10, 5))\n",
    "# train = .85\n",
    "# test  = .83"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
